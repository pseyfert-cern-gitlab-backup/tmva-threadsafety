// Class: ReadMLP_Forward2ndLoop
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP_Forward2ndLoop
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/02       [394754]
Creator        : tnikodem
Date           : Thu May 12 17:44:50 2016
Host           : Linux lcgapp-slc6-physical1.cern.ch 2.6.32-573.8.1.el6.x86_64 #1 SMP Wed Nov 11 15:27:45 CET 2015 x86_64 x86_64 x86_64 GNU/Linux
Dir            : /afs/cern.ch/work/t/tnikodem/work/FwdFitParams2/NeedforSpeed_Forward/Brunel_v49r1/runBrunel/TMVA2
Training events: 176444
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "1500" [Number of training cycles]
HiddenLayers: "N+3,N+2,N" [Specification of hidden layer architecture]
NeuronType: "ReLU" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g., "D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)"]
H: "True" [Print method-specific help message]
LearningRate: "1.000000e-02" [ANN learning rate parameter]
DecayRate: "5.000000e-03" [Decay rate for learning parameter]
TestRate: "5" [Test for overtraining performed at each #th epochs]
UseRegulator: "False" [Use regulator to avoid over-training]
# Default:
RandomSeed: "1" [Random seed for initial synapse weights (0 means unique seed for each run; default value '1')]
NeuronInputType: "sum" [Neuron input function type]
VerbosityLevel: "Default" [Verbosity level]
CreateMVAPdfs: "False" [Create PDFs for classifier outputs (signal and background)]
IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]
TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm (BFGS), or Genetic Algorithm (GA - slower and worse)]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output file!)]
Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch]
SamplingEpoch: "1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than before) are multiplied with SamplingImportance, else they are divided.]
SamplingTraining: "True" [The training sample is sampled]
SamplingTesting: "False" [The testing sample is sampled]
ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence check is turned off)]
ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means automatic convergence check is turned off)]
UpdateLimit: "10000" [Maximum times of regulator update]
CalculateErrors: "False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an MVA value]
WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 7
nPlanes                       nPlanes                       nPlanes                       nPlanes                                                         'I'    [10,12]
dSlope                        p                             p                             p                                                               'F'    [-0.000889532791916,0.000888269452844]
dp                            dp                            dp                            dp                                                              'F'    [-0.00094475003425,0.000963236612733]
slope2                        slope2                        slope2                        slope2                                                          'F'    [9.59152966971e-05,0.171912387013]
dby                           dby                           dby                           dby                                                             'F'    [-0.537947356701,0.509115695953]
dbx                           dbx                           dbx                           dbx                                                             'F'    [-0.0266773104668,0.0260969996452]
day                           day                           day                           day                                                             'F'    [-191.514282227,195.880737305]
NSpec 0


============================================================================ */

#include <vector>
#include <cmath>
#include <string>
#include <iostream>

#ifndef IClassifierReader__def
#define IClassifierReader__def

class IClassifierReader {

 public:

   // constructor
   IClassifierReader() {}
   virtual ~IClassifierReader() {}

   // return classifier response
   virtual float GetMvaValue( const std::vector<float>& inputValues ) const = 0;

};

#endif

class ReadMLP_Forward2ndLoop : public IClassifierReader {

 public:

   // constructor
   ReadMLP_Forward2ndLoop( const std::vector<std::string>& theInputVars )
      : IClassifierReader(),
        fClassName( "ReadMLP_Forward2ndLoop" ),
        fNvars( 7 )
   {
      // the training input variables
      const char* inputVars[] = { "nPlanes", "dSlope", "dp", "slope2", "dby", "dbx", "day" };

      // sanity checks
      if (theInputVars.size() <= 0) {
         std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
         return;
      }

      if (theInputVars.size() != fNvars) {
         std::cout << "Problem in class \"" << fClassName << "\": mismatch in number of input values: "
                   << theInputVars.size() << " != " << fNvars << std::endl;
         return;
      }

      // validate input variables
      for (size_t ivar = 0; ivar < theInputVars.size(); ivar++) {
         if (theInputVars[ivar] != inputVars[ivar]) {
            std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                      << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar] << std::endl;
         return;
         }
      }

      // initialize constants
      Initialize();

      // initialize transformation
      InitTransform_1();
   }

   // destructor
   virtual ~ReadMLP_Forward2ndLoop() {
      Clear(); // method-specific
   }

   // the classifier response
   // "inputValues" is a vector of input values in the same order as the
   // variables given to the constructor
   // WARNING: inputVariables will be modified
   inline float GetMvaValue( const std::vector<float>& iV ) const override
   {
     std::vector<float> localcopy = iV;
      //Normalize input
      Transform_1( localcopy);

      return GetMvaValue__( localcopy );
   }


 private:

  // method-specific destructor
  inline void Clear()
  {
     // clean up the arrays
     for (int lIdx = 0; lIdx < 5; lIdx++) {
        delete[] fWeights[lIdx];
     }
  }

  // input variable transformation

  float fMin_1[3][7];
  float fMax_1[3][7];

  inline void InitTransform_1()
  {
     // Normalization transformation, initialisation
     fMin_1[0][0] = 10;
     fMax_1[0][0] = 12;
     fMin_1[1][0] = 10;
     fMax_1[1][0] = 12;
     fMin_1[2][0] = 10;
     fMax_1[2][0] = 12;
     fMin_1[0][1] = -0.000751986168325;
     fMax_1[0][1] = 0.000730101251975;
     fMin_1[1][1] = -0.000889532791916;
     fMax_1[1][1] = 0.000888269452844;
     fMin_1[2][1] = -0.000889532791916;
     fMax_1[2][1] = 0.000888269452844;
     fMin_1[0][2] = -0.000719623989426;
     fMax_1[0][2] = 0.000641609425656;
     fMin_1[1][2] = -0.00094475003425;
     fMax_1[1][2] = 0.000963236612733;
     fMin_1[2][2] = -0.00094475003425;
     fMax_1[2][2] = 0.000963236612733;
     fMin_1[0][3] = 0.000119972355606;
     fMax_1[0][3] = 0.171912387013;
     fMin_1[1][3] = 9.59152966971e-05;
     fMax_1[1][3] = 0.161721900105;
     fMin_1[2][3] = 9.59152966971e-05;
     fMax_1[2][3] = 0.171912387013;
     fMin_1[0][4] = -0.359134197235;
     fMax_1[0][4] = 0.203789561987;
     fMin_1[1][4] = -0.537947356701;
     fMax_1[1][4] = 0.509115695953;
     fMin_1[2][4] = -0.537947356701;
     fMax_1[2][4] = 0.509115695953;
     fMin_1[0][5] = -0.0192034840584;
     fMax_1[0][5] = 0.019477725029;
     fMin_1[1][5] = -0.0266773104668;
     fMax_1[1][5] = 0.0260969996452;
     fMin_1[2][5] = -0.0266773104668;
     fMax_1[2][5] = 0.0260969996452;
     fMin_1[0][6] = -191.514282227;
     fMax_1[0][6] = 149.822387695;
     fMin_1[1][6] = -177.097839355;
     fMax_1[1][6] = 195.880737305;
     fMin_1[2][6] = -191.514282227;
     fMax_1[2][6] = 195.880737305;
  }

  inline void Transform_1( std::vector<float>& iv) const
  {
     const int cls = 2; //what are the other???
     const int nVar = 7;

     for (int ivar=0;ivar<nVar;ivar++) {
        const float offset = fMin_1[cls][ivar];
        const float scale  = 1.0/(fMax_1[cls][ivar]-fMin_1[cls][ivar]); //TODO speed this up. but then not easy to update :(
        iv[ivar] = (iv[ivar]-offset)*scale * 2. - 1.;
     }
  }

  // common member variables
  const char* fClassName;
  const size_t fNvars;

  // initialize internal variables
  inline void Initialize()
  {
     // build network structure
     fLayers = 5;
     fLayerSize[0] = 8;  fWeights[0] = new float[8];
     fLayerSize[1] = 11; fWeights[1] = new float[11];
     fLayerSize[2] = 10; fWeights[2] = new float[10];
     fLayerSize[3] = 8;  fWeights[3] = new float[8];
     fLayerSize[4] = 1;  fWeights[4] = new float[1];
     // weight matrix from layer 0 to 1
     fWeightMatrix0to1[0][0] = -0.363682357093314;
     fWeightMatrix0to1[1][0] = -0.0489472213380593;
     fWeightMatrix0to1[2][0] = 0.0260684359162034;
     fWeightMatrix0to1[3][0] = -0.0183195135886001;
     fWeightMatrix0to1[4][0] = -0.0435597940879404;
     fWeightMatrix0to1[5][0] = -0.409695697569545;
     fWeightMatrix0to1[6][0] = 0.083823608603888;
     fWeightMatrix0to1[7][0] = 2.7884563303233;
     fWeightMatrix0to1[8][0] = -0.0327817823630005;
     fWeightMatrix0to1[9][0] = -0.152166863476521;
     fWeightMatrix0to1[0][1] = -8.02569746070074;
     fWeightMatrix0to1[1][1] = -1.57566743031828;
     fWeightMatrix0to1[2][1] = -8.74113304358939;
     fWeightMatrix0to1[3][1] = -15.1660820931205;
     fWeightMatrix0to1[4][1] = -3.7244859171386;
     fWeightMatrix0to1[5][1] = -5.85502710533626;
     fWeightMatrix0to1[6][1] = -1.21441194925294;
     fWeightMatrix0to1[7][1] = 0.159766452843058;
     fWeightMatrix0to1[8][1] = -5.13496215182161;
     fWeightMatrix0to1[9][1] = -1.9665293685506;
     fWeightMatrix0to1[0][2] = -3.87259609677237;
     fWeightMatrix0to1[1][2] = -13.7463874097183;
     fWeightMatrix0to1[2][2] = -4.66960984157945e-05;
     fWeightMatrix0to1[3][2] = -0.184966358069029;
     fWeightMatrix0to1[4][2] = -2.98033381620024;
     fWeightMatrix0to1[5][2] = -0.646993912152853;
     fWeightMatrix0to1[6][2] = 27.1148466663061;
     fWeightMatrix0to1[7][2] = 0.197114374537912;
     fWeightMatrix0to1[8][2] = -3.91787190684213;
     fWeightMatrix0to1[9][2] = -0.512554051079575;
     fWeightMatrix0to1[0][3] = 4.95541028582906;
     fWeightMatrix0to1[1][3] = -0.982579271133555;
     fWeightMatrix0to1[2][3] = 2.25583004410242;
     fWeightMatrix0to1[3][3] = -2.48764727093388;
     fWeightMatrix0to1[4][3] = -9.71388568465769;
     fWeightMatrix0to1[5][3] = 4.39989922354738;
     fWeightMatrix0to1[6][3] = -1.99668262303782;
     fWeightMatrix0to1[7][3] = 9.12999515405744;
     fWeightMatrix0to1[8][3] = -7.6391652044467;
     fWeightMatrix0to1[9][3] = 1.9417568161743;
     fWeightMatrix0to1[0][4] = 0.693020864624262;
     fWeightMatrix0to1[1][4] = 1.03896782947591;
     fWeightMatrix0to1[2][4] = 0.42106865840627;
     fWeightMatrix0to1[3][4] = -1.10974045620275;
     fWeightMatrix0to1[4][4] = -0.10554329100342;
     fWeightMatrix0to1[5][4] = -19.467568592815;
     fWeightMatrix0to1[6][4] = 0.541954630475377;
     fWeightMatrix0to1[7][4] = 0.672219357358115;
     fWeightMatrix0to1[8][4] = 2.54286051035417;
     fWeightMatrix0to1[9][4] = 36.8873831511543;
     fWeightMatrix0to1[0][5] = 3.29431313466074;
     fWeightMatrix0to1[1][5] = 11.664575333949;
     fWeightMatrix0to1[2][5] = 3.43714144134435;
     fWeightMatrix0to1[3][5] = 1.85706790726995;
     fWeightMatrix0to1[4][5] = -3.07833615212947;
     fWeightMatrix0to1[5][5] = -0.886964190370458;
     fWeightMatrix0to1[6][5] = -1.40606434245474;
     fWeightMatrix0to1[7][5] = -0.0894519016668799;
     fWeightMatrix0to1[8][5] = -3.24990700112274;
     fWeightMatrix0to1[9][5] = -0.167564019764147;
     fWeightMatrix0to1[0][6] = 0.0338826751202223;
     fWeightMatrix0to1[1][6] = -1.92783872591996;
     fWeightMatrix0to1[2][6] = 1.1775088910565;
     fWeightMatrix0to1[3][6] = -0.168732040045552;
     fWeightMatrix0to1[4][6] = 14.7029039282644;
     fWeightMatrix0to1[5][6] = 1.15730572575181;
     fWeightMatrix0to1[6][6] = 0.784103977726078;
     fWeightMatrix0to1[7][6] = -0.486152580769206;
     fWeightMatrix0to1[8][6] = -8.15354484425858;
     fWeightMatrix0to1[9][6] = -1.83517996941011;
     fWeightMatrix0to1[0][7] = -1.47411332800668;
     fWeightMatrix0to1[1][7] = -1.41692905398734;
     fWeightMatrix0to1[2][7] = -0.102182533294306;
     fWeightMatrix0to1[3][7] = -2.4103624442092;
     fWeightMatrix0to1[4][7] = -7.57107586584993;
     fWeightMatrix0to1[5][7] = 6.29607332908425;
     fWeightMatrix0to1[6][7] = -1.71383034420142;
     fWeightMatrix0to1[7][7] = 6.14511047364846;
     fWeightMatrix0to1[8][7] = -3.93791385193052;
     fWeightMatrix0to1[9][7] = 1.48072208584235;
     // weight matrix from layer 1 to 2
     fWeightMatrix1to2[0][0] = 4.12677463873458;
     fWeightMatrix1to2[1][0] = 0.95694975193799;
     fWeightMatrix1to2[2][0] = 2.68236250583668;
     fWeightMatrix1to2[3][0] = -1.60535968633654;
     fWeightMatrix1to2[4][0] = -5.37872915673067;
     fWeightMatrix1to2[5][0] = -0.664854638721318;
     fWeightMatrix1to2[6][0] = 4.16620047731897;
     fWeightMatrix1to2[7][0] = 0.799035782689849;
     fWeightMatrix1to2[8][0] = -0.225088888088133;
     fWeightMatrix1to2[0][1] = 2.09374462359668;
     fWeightMatrix1to2[1][1] = -1.51413159263093;
     fWeightMatrix1to2[2][1] = -2.14016618792657;
     fWeightMatrix1to2[3][1] = -0.158046673411734;
     fWeightMatrix1to2[4][1] = 0.373618282718242;
     fWeightMatrix1to2[5][1] = 1.95941843491082;
     fWeightMatrix1to2[6][1] = -0.160149534664951;
     fWeightMatrix1to2[7][1] = 0.0942699565784549;
     fWeightMatrix1to2[8][1] = -2.28998444992079;
     fWeightMatrix1to2[0][2] = -2.03361729487953;
     fWeightMatrix1to2[1][2] = 0.246934403528477;
     fWeightMatrix1to2[2][2] = 1.23912692639827;
     fWeightMatrix1to2[3][2] = -1.24584976309099;
     fWeightMatrix1to2[4][2] = 0.232791533134803;
     fWeightMatrix1to2[5][2] = -0.0984830387133716;
     fWeightMatrix1to2[6][2] = -1.35662278212464;
     fWeightMatrix1to2[7][2] = -0.618030631899878;
     fWeightMatrix1to2[8][2] = 3.31051938706427;
     fWeightMatrix1to2[0][3] = -2.24216091836216;
     fWeightMatrix1to2[1][3] = -2.0741279159171;
     fWeightMatrix1to2[2][3] = 1.58905650835314;
     fWeightMatrix1to2[3][3] = -2.24329177699312;
     fWeightMatrix1to2[4][3] = -1.32954165411477;
     fWeightMatrix1to2[5][3] = -0.962141953588846;
     fWeightMatrix1to2[6][3] = 0.553473134087373;
     fWeightMatrix1to2[7][3] = -0.221879659733536;
     fWeightMatrix1to2[8][3] = 1.37972245088897;
     fWeightMatrix1to2[0][4] = 1.61803204646002;
     fWeightMatrix1to2[1][4] = 0.149486276956427;
     fWeightMatrix1to2[2][4] = 0.241106175418173;
     fWeightMatrix1to2[3][4] = -0.871515215778542;
     fWeightMatrix1to2[4][4] = -2.15288250173632;
     fWeightMatrix1to2[5][4] = 0.704719162938588;
     fWeightMatrix1to2[6][4] = -1.85276053353347;
     fWeightMatrix1to2[7][4] = 0.152763398436487;
     fWeightMatrix1to2[8][4] = -2.1381756643797;
     fWeightMatrix1to2[0][5] = 0.451165224481025;
     fWeightMatrix1to2[1][5] = -0.197440200947197;
     fWeightMatrix1to2[2][5] = 0.799551550055436;
     fWeightMatrix1to2[3][5] = -1.18507878989424;
     fWeightMatrix1to2[4][5] = 0.736442854852866;
     fWeightMatrix1to2[5][5] = -0.778210248681898;
     fWeightMatrix1to2[6][5] = 1.0184279963991;
     fWeightMatrix1to2[7][5] = 3.02152020947048;
     fWeightMatrix1to2[8][5] = 1.44488951951168;
     fWeightMatrix1to2[0][6] = 2.11855113889747;
     fWeightMatrix1to2[1][6] = 1.13030384892463;
     fWeightMatrix1to2[2][6] = -0.409760081875493;
     fWeightMatrix1to2[3][6] = -1.14736280086342;
     fWeightMatrix1to2[4][6] = -0.177061437738292;
     fWeightMatrix1to2[5][6] = 0.949169465138031;
     fWeightMatrix1to2[6][6] = -0.3285355519552;
     fWeightMatrix1to2[7][6] = -0.0793765092190401;
     fWeightMatrix1to2[8][6] = -2.37773041909011;
     fWeightMatrix1to2[0][7] = -0.750831940769777;
     fWeightMatrix1to2[1][7] = 0.178009942769106;
     fWeightMatrix1to2[2][7] = -1.02237486835902;
     fWeightMatrix1to2[3][7] = 0.967964323610706;
     fWeightMatrix1to2[4][7] = -4.11405845648907;
     fWeightMatrix1to2[5][7] = 0.970851461678289;
     fWeightMatrix1to2[6][7] = -0.314542140867855;
     fWeightMatrix1to2[7][7] = 0.0267235503842247;
     fWeightMatrix1to2[8][7] = 0.777583791043133;
     fWeightMatrix1to2[0][8] = -0.0766468428753576;
     fWeightMatrix1to2[1][8] = -1.29672432665597;
     fWeightMatrix1to2[2][8] = -1.07091840218592;
     fWeightMatrix1to2[3][8] = -0.557257692690983;
     fWeightMatrix1to2[4][8] = 2.56309454383188;
     fWeightMatrix1to2[5][8] = 2.59834254898381;
     fWeightMatrix1to2[6][8] = 0.451467225870388;
     fWeightMatrix1to2[7][8] = -0.271472478418115;
     fWeightMatrix1to2[8][8] = 0.322847172776879;
     fWeightMatrix1to2[0][9] = 1.50513181890796;
     fWeightMatrix1to2[1][9] = -1.74229624893957;
     fWeightMatrix1to2[2][9] = -1.45656061974725;
     fWeightMatrix1to2[3][9] = -0.963316737364389;
     fWeightMatrix1to2[4][9] = -0.501558620705004;
     fWeightMatrix1to2[5][9] = -1.08768021305589;
     fWeightMatrix1to2[6][9] = 1.10289926307001;
     fWeightMatrix1to2[7][9] = -7.54827898955444;
     fWeightMatrix1to2[8][9] = -3.97260031491365;
     fWeightMatrix1to2[0][10] = -4.25444399915558;
     fWeightMatrix1to2[1][10] = 2.7785088087099;
     fWeightMatrix1to2[2][10] = -0.855599304985592;
     fWeightMatrix1to2[3][10] = 6.31732858642786;
     fWeightMatrix1to2[4][10] = -6.38927142032414;
     fWeightMatrix1to2[5][10] = -8.68408961949098;
     fWeightMatrix1to2[6][10] = -0.966303858068243;
     fWeightMatrix1to2[7][10] = 0.956475297634228;
     fWeightMatrix1to2[8][10] = 4.9830363220156;
     // weight matrix from layer 2 to 3
     fWeightMatrix2to3[0][0] = -0.359705221916529;
     fWeightMatrix2to3[1][0] = -1.33108343881925;
     fWeightMatrix2to3[2][0] = -0.254173419631061;
     fWeightMatrix2to3[3][0] = 0.844269213532595;
     fWeightMatrix2to3[4][0] = 0.229231897608563;
     fWeightMatrix2to3[5][0] = -0.824395661056368;
     fWeightMatrix2to3[6][0] = 0.305257835074692;
     fWeightMatrix2to3[0][1] = 0.0488169282722191;
     fWeightMatrix2to3[1][1] = 0.195648713620308;
     fWeightMatrix2to3[2][1] = 0.147800158020385;
     fWeightMatrix2to3[3][1] = -0.980206859398799;
     fWeightMatrix2to3[4][1] = -0.733677868364601;
     fWeightMatrix2to3[5][1] = 0.941940627461568;
     fWeightMatrix2to3[6][1] = -0.231109764490389;
     fWeightMatrix2to3[0][2] = 0.844470069798132;
     fWeightMatrix2to3[1][2] = -0.900908680241791;
     fWeightMatrix2to3[2][2] = 0.594168030958934;
     fWeightMatrix2to3[3][2] = -1.25436099188948;
     fWeightMatrix2to3[4][2] = -0.351556746084157;
     fWeightMatrix2to3[5][2] = 0.187005315757542;
     fWeightMatrix2to3[6][2] = 0.213681768782238;
     fWeightMatrix2to3[0][3] = -2.33420223621305;
     fWeightMatrix2to3[1][3] = -0.593859756778516;
     fWeightMatrix2to3[2][3] = 0.671475416265333;
     fWeightMatrix2to3[3][3] = -0.721341813887584;
     fWeightMatrix2to3[4][3] = 0.506023812700976;
     fWeightMatrix2to3[5][3] = 1.67673691416136;
     fWeightMatrix2to3[6][3] = -2.124662366003;
     fWeightMatrix2to3[0][4] = 0.381704742406321;
     fWeightMatrix2to3[1][4] = -1.10907997416722;
     fWeightMatrix2to3[2][4] = 0.305601285490167;
     fWeightMatrix2to3[3][4] = 1.8290628215884;
     fWeightMatrix2to3[4][4] = 1.59684613442337;
     fWeightMatrix2to3[5][4] = -1.63529379893428;
     fWeightMatrix2to3[6][4] = -1.10871648278323;
     fWeightMatrix2to3[0][5] = 0.887026509690821;
     fWeightMatrix2to3[1][5] = -2.41063403571978;
     fWeightMatrix2to3[2][5] = 0.876840414959149;
     fWeightMatrix2to3[3][5] = -0.41423276945129;
     fWeightMatrix2to3[4][5] = 0.695524010798871;
     fWeightMatrix2to3[5][5] = -0.332151547886367;
     fWeightMatrix2to3[6][5] = -0.954094195036293;
     fWeightMatrix2to3[0][6] = -0.87673131357239;
     fWeightMatrix2to3[1][6] = -0.589817661983923;
     fWeightMatrix2to3[2][6] = 0.607084417156363;
     fWeightMatrix2to3[3][6] = -0.24324056006996;
     fWeightMatrix2to3[4][6] = 0.553432299251619;
     fWeightMatrix2to3[5][6] = -2.00310191419927;
     fWeightMatrix2to3[6][6] = -0.972887313360374;
     fWeightMatrix2to3[0][7] = -0.00952422797350496;
     fWeightMatrix2to3[1][7] = -1.15043208722106;
     fWeightMatrix2to3[2][7] = 0.646482949790799;
     fWeightMatrix2to3[3][7] = 0.541747178992036;
     fWeightMatrix2to3[4][7] = 0.856232431477478;
     fWeightMatrix2to3[5][7] = 0.101887676904865;
     fWeightMatrix2to3[6][7] = -0.109826032567475;
     fWeightMatrix2to3[0][8] = -0.330563362034855;
     fWeightMatrix2to3[1][8] = 0.485458770361598;
     fWeightMatrix2to3[2][8] = -0.783414671398449;
     fWeightMatrix2to3[3][8] = -0.891742600447044;
     fWeightMatrix2to3[4][8] = -1.22573773056992;
     fWeightMatrix2to3[5][8] = -0.861200300490884;
     fWeightMatrix2to3[6][8] = -2.77863046899075;
     fWeightMatrix2to3[0][9] = -3.21025190679825;
     fWeightMatrix2to3[1][9] = 5.05459734017371;
     fWeightMatrix2to3[2][9] = 1.42880862651111;
     fWeightMatrix2to3[3][9] = 1.88916122723334;
     fWeightMatrix2to3[4][9] = -4.61241600524382;
     fWeightMatrix2to3[5][9] = -3.90792395129795;
     fWeightMatrix2to3[6][9] = 1.51275256453558;
     // weight matrix from layer 3 to 4
     fWeightMatrix3to4[0][0] = 0.490641348474588;
     fWeightMatrix3to4[0][1] = 0.483067099717087;
     fWeightMatrix3to4[0][2] = -0.541242670164575;
     fWeightMatrix3to4[0][3] = -0.58053622359768;
     fWeightMatrix3to4[0][4] = 0.425277394090215;
     fWeightMatrix3to4[0][5] = 0.469384698563024;
     fWeightMatrix3to4[0][6] = -0.472826385903612;
     fWeightMatrix3to4[0][7] = 1.75985681009234;
  }

  inline float GetMvaValue__( const std::vector<float>& inputValues ) const
  {
    //TODO check auto vectorization here: 'not vectorized: unsupported use in stmt' , 'Unsupported pattern'

     for (int l=0; l<fLayers; l++)
        for (int i=0; i<fLayerSize[l]; i++) fWeights[l][i]=0.f;

     for (int l=0; l<fLayers-1; l++)
        fWeights[l][fLayerSize[l]-1]=1.f;

     for (int i=0; i<7; i++)
        fWeights[0][i]=inputValues[i];

     // layer 0 to 1
     for (int o=0; o<10; o++) {
        for (int i=0; i<8; i++) {
           float inputVal = fWeightMatrix0to1[o][i] * fWeights[0][i];
           fWeights[1][o] += inputVal;
        }
        fWeights[1][o] = ActivationFnc(fWeights[1][o]);
     }
     // layer 1 to 2
     for (int o=0; o<9; o++) {
        for (int i=0; i<11; i++) {
           float inputVal = fWeightMatrix1to2[o][i] * fWeights[1][i];
           fWeights[2][o] += inputVal;
        }
        fWeights[2][o] = ActivationFnc(fWeights[2][o]);
     }
     // layer 2 to 3
     for (int o=0; o<7; o++) {
        for (int i=0; i<10; i++) {
           float inputVal = fWeightMatrix2to3[o][i] * fWeights[2][i];
           fWeights[3][o] += inputVal;
        }
        fWeights[3][o] = ActivationFnc(fWeights[3][o]);
     }
     // layer 3 to 4
     for (int i=0; i<8; i++) {
       float inputVal = fWeightMatrix3to4[0][i] * fWeights[3][i];
       fWeights[4][0] += inputVal;
     }
     return OutputActivationFnc(fWeights[4][0]);

  }

  inline float ActivationFnc(float x) const {
     // rectified linear unit
     return x*(x>0);
  }

  //TODO do we need exp here? can we live with pure x output?!
  inline float OutputActivationFnc(float x) const {
     // sigmoid
     return 1.0/(1.0+exp(-x));
  }


  int fLayers;
  int fLayerSize[5];
  float fWeightMatrix0to1[11][8];   // weight matrix from layer 0 to 1
  float fWeightMatrix1to2[10][11];   // weight matrix from layer 1 to 2
  float fWeightMatrix2to3[8][10];   // weight matrix from layer 2 to 3
  float fWeightMatrix3to4[1][8];   // weight matrix from layer 3 to 4

  float * fWeights[5];

};
