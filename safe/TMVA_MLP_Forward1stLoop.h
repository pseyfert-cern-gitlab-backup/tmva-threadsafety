// Class: ReadMLP_Forward1stLoop
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : MLP::MLP_Forward1stLoop
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.06/00       [394752]
Creator        : tnikodem
Date           : Thu May 12 23:31:40 2016
Host           : Linux lcgapp-slc6-physical1.cern.ch 2.6.32-573.8.1.el6.x86_64 #1 SMP Wed Nov 11 15:27:45 CET 2015 x86_64 x86_64 x86_64 GNU/Linux
Dir            : /afs/cern.ch/work/t/tnikodem/work/FwdFitParams2/NeedforSpeed_Forward/Brunel_v49r1/runBrunel/TMVA
Training events: 1331762
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
NCycles: "1500" [Number of training cycles]
HiddenLayers: "N+3,N+2,N" [Specification of hidden layer architecture]
NeuronType: "ReLU" [Neuron activation function type]
EstimatorType: "CE" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
VarTransform: "N" [List of variable transformations performed before training, e.g., "D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)"]
H: "True" [Print method-specific help message]
LearningRate: "1.000000e-02" [ANN learning rate parameter]
DecayRate: "5.000000e-03" [Decay rate for learning parameter]
TestRate: "5" [Test for overtraining performed at each #th epochs]
UseRegulator: "False" [Use regulator to avoid over-training]
# Default:
RandomSeed: "1" [Random seed for initial synapse weights (0 means unique seed for each run; default value '1')]
NeuronInputType: "sum" [Neuron input function type]
VerbosityLevel: "Default" [Verbosity level]
CreateMVAPdfs: "False" [Create PDFs for classifier outputs (signal and background)]
IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]
TrainingMethod: "BP" [Train with Back-Propagation (BP), BFGS Algorithm (BFGS), or Genetic Algorithm (GA - slower and worse)]
EpochMonitoring: "False" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output file!)]
Sampling: "1.000000e+00" [Only 'Sampling' (randomly selected) events are trained each epoch]
SamplingEpoch: "1.000000e+00" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]
SamplingImportance: "1.000000e+00" [ The sampling weights of events in epochs which successful (worse estimator than before) are multiplied with SamplingImportance, else they are divided.]
SamplingTraining: "True" [The training sample is sampled]
SamplingTesting: "False" [The testing sample is sampled]
ResetStep: "50" [How often BFGS should reset history]
Tau: "3.000000e+00" [LineSearch "size step"]
BPMode: "sequential" [Back-propagation learning mode: sequential or batch]
BatchSize: "-1" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]
ConvergenceImprove: "1.000000e-30" [Minimum improvement which counts as improvement (<0 means automatic convergence check is turned off)]
ConvergenceTests: "-1" [Number of steps (without improvement) required for convergence (<0 means automatic convergence check is turned off)]
UpdateLimit: "10000" [Maximum times of regulator update]
CalculateErrors: "False" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an MVA value]
WeightRange: "1.000000e+00" [Take the events for the estimator calculations from small deviations from the desired value to large deviations only over the weight range]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 7
nPlanes                       nPlanes                       nPlanes                       nPlanes                                                         'I'    [10,12]
dSlope                        p                             p                             p                                                               'F'    [-0.000876020232681,0.000809539866168]
dp                            dp                            dp                            dp                                                              'F'    [-0.000959908822551,0.000927863351535]
slope2                        slope2                        slope2                        slope2                                                          'F'    [6.83568359818e-05,0.171713337302]
dby                           dby                           dby                           dby                                                             'F'    [-0.502270042896,0.598416030407]
dbx                           dbx                           dbx                           dbx                                                             'F'    [-0.0183157920837,0.0351094007492]
day                           day                           day                           day                                                             'F'    [-178.507324219,188.444091797]
NSpec 0


============================================================================ */

class IClassifierReader {

 public:

   // constructor
   IClassifierReader() {}
   virtual ~IClassifierReader() {}

   // return classifier response
   virtual float GetMvaValue( const std::vector<float>& inputValues ) const = 0;

};

class ReadMLP_Forward1stLoop : public IClassifierReader {

 public:

   // constructor
   ReadMLP_Forward1stLoop( const std::vector<std::string>& theInputVars )
      : IClassifierReader(),
        fClassName( "ReadMLP_Forward1stLoop" ),
        fNvars( 7 )
   {
      // the training input variables
      const char* inputVars[] = { "nPlanes", "dSlope", "dp", "slope2", "dby", "dbx", "day" };

      // sanity checks
      if (theInputVars.size() <= 0) {
         std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
         return;
      }

      if (theInputVars.size() != fNvars) {
         std::cout << "Problem in class \"" << fClassName << "\": mismatch in number of input values: "
                   << theInputVars.size() << " != " << fNvars << std::endl;
         return;
      }

      // validate input variables
      for (size_t ivar = 0; ivar < theInputVars.size(); ivar++) {
         if (theInputVars[ivar] != inputVars[ivar]) {
            std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                      << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar] << std::endl;
         return;
         }
      }

      // initialize constants
      Initialize();

      // initialize transformation
      InitTransform_1();
   }

   // destructor
   virtual ~ReadMLP_Forward1stLoop() {
      Clear(); // method-specific
   }

   // the classifier response
   // "inputValues" is a vector of input values in the same order as the
   // variables given to the constructor
   // WARNING: inputVariables will be modified
   inline float GetMvaValue( const std::vector<float>& iV ) const override
   {
      std::vector<float> copy = iV;
      //Normalize input
      Transform_1( copy);

      return GetMvaValue__( copy );
   }



  // method-specific destructor
  inline void Clear()
  {
  }

  // input variable transformation

  float fMin_1[3][7];
  float fMax_1[3][7];

  inline void InitTransform_1()
  {
     // Normalization transformation, initialisation
     fMin_1[0][0] = 10;
     fMax_1[0][0] = 12;
     fMin_1[1][0] = 10;
     fMax_1[1][0] = 12;
     fMin_1[2][0] = 10;
     fMax_1[2][0] = 12;
     fMin_1[0][1] = -0.000693323207088;
     fMax_1[0][1] = 0.000661431695335;
     fMin_1[1][1] = -0.000876020232681;
     fMax_1[1][1] = 0.000809539866168;
     fMin_1[2][1] = -0.000876020232681;
     fMax_1[2][1] = 0.000809539866168;
     fMin_1[0][2] = -0.000726951810066;
     fMax_1[0][2] = 0.000800277863164;
     fMin_1[1][2] = -0.000959908822551;
     fMax_1[1][2] = 0.000927863351535;
     fMin_1[2][2] = -0.000959908822551;
     fMax_1[2][2] = 0.000927863351535;
     fMin_1[0][3] = 8.73411772773e-05;
     fMax_1[0][3] = 0.161471515894;
     fMin_1[1][3] = 6.83568359818e-05;
     fMax_1[1][3] = 0.171713337302;
     fMin_1[2][3] = 6.83568359818e-05;
     fMax_1[2][3] = 0.171713337302;
     fMin_1[0][4] = -0.41890129447;
     fMax_1[0][4] = 0.25404638052;
     fMin_1[1][4] = -0.502270042896;
     fMax_1[1][4] = 0.598416030407;
     fMin_1[2][4] = -0.502270042896;
     fMax_1[2][4] = 0.598416030407;
     fMin_1[0][5] = -0.0150038003922;
     fMax_1[0][5] = 0.0172826647758;
     fMin_1[1][5] = -0.0183157920837;
     fMax_1[1][5] = 0.0351094007492;
     fMin_1[2][5] = -0.0183157920837;
     fMax_1[2][5] = 0.0351094007492;
     fMin_1[0][6] = -139.267822266;
     fMax_1[0][6] = 150.771850586;
     fMin_1[1][6] = -178.507324219;
     fMax_1[1][6] = 188.444091797;
     fMin_1[2][6] = -178.507324219;
     fMax_1[2][6] = 188.444091797;
  }

  inline void Transform_1( std::vector<float>& iv) const
  {
     const int cls = 2; //what are the other???
     const int nVar = 7;

     for (int ivar=0;ivar<nVar;ivar++) {
        const float offset = fMin_1[cls][ivar];
        const float scale  = 1.0/(fMax_1[cls][ivar]-fMin_1[cls][ivar]); //TODO speed this up. but then not easy to update :(
        iv[ivar] = (iv[ivar]-offset)*scale * 2. - 1.;
     }
  }

  // common member variables
  const char* fClassName;
  const size_t fNvars;

  // initialize internal variables
  inline void Initialize()
  {
     // build network structure
     fLayers = 5;
     fLayerSize[0] = 8;
     fLayerSize[1] = 11;
     fLayerSize[2] = 10;
     fLayerSize[3] = 8;
     fLayerSize[4] = 1;
     // weight matrix from layer 0 to 1
     fWeightMatrix0to1[0][0] = 0.156511290357907;
     fWeightMatrix0to1[1][0] = 5.859677434496;
     fWeightMatrix0to1[2][0] = 0.0403216550547087;
     fWeightMatrix0to1[3][0] = 0.118942942089809;
     fWeightMatrix0to1[4][0] = -0.0524130927108231;
     fWeightMatrix0to1[5][0] = -0.0615780376417546;
     fWeightMatrix0to1[6][0] = 0.100115445436162;
     fWeightMatrix0to1[7][0] = 0.0612125106565374;
     fWeightMatrix0to1[8][0] = 0.0351187781245128;
     fWeightMatrix0to1[9][0] = -7.80295272542051;
     fWeightMatrix0to1[0][1] = -15.9012468978926;
     fWeightMatrix0to1[1][1] = 0.103375966424631;
     fWeightMatrix0to1[2][1] = 2.84181299733342;
     fWeightMatrix0to1[3][1] = 14.0872677784526;
     fWeightMatrix0to1[4][1] = 0.00656033420416158;
     fWeightMatrix0to1[5][1] = -2.05049322142208;
     fWeightMatrix0to1[6][1] = -0.476796624482398;
     fWeightMatrix0to1[7][1] = 5.34178801874048;
     fWeightMatrix0to1[8][1] = -2.03842114206132;
     fWeightMatrix0to1[9][1] = 1.61796425984333;
     fWeightMatrix0to1[0][2] = -0.626716696225122;
     fWeightMatrix0to1[1][2] = 0.293994980752188;
     fWeightMatrix0to1[2][2] = -5.8378296035011;
     fWeightMatrix0to1[3][2] = 1.88698468349361;
     fWeightMatrix0to1[4][2] = 2.59126699636858;
     fWeightMatrix0to1[5][2] = 2.76426352177771;
     fWeightMatrix0to1[6][2] = 35.7389195659674;
     fWeightMatrix0to1[7][2] = 15.3804699396489;
     fWeightMatrix0to1[8][2] = -34.4263535379079;
     fWeightMatrix0to1[9][2] = 0.217589935429331;
     fWeightMatrix0to1[0][3] = 5.04491816907169;
     fWeightMatrix0to1[1][3] = 4.99503349862526;
     fWeightMatrix0to1[2][3] = -9.72920646551164;
     fWeightMatrix0to1[3][3] = 7.30206859198975;
     fWeightMatrix0to1[4][3] = -3.63707417363857;
     fWeightMatrix0to1[5][3] = -5.86739614942788;
     fWeightMatrix0to1[6][3] = -4.13376983329174;
     fWeightMatrix0to1[7][3] = -8.62681866836684;
     fWeightMatrix0to1[8][3] = -2.35484344660064;
     fWeightMatrix0to1[9][3] = 15.6094338591807;
     fWeightMatrix0to1[0][4] = -4.80684731442771;
     fWeightMatrix0to1[1][4] = 1.42397639088434;
     fWeightMatrix0to1[2][4] = 18.6262099897472;
     fWeightMatrix0to1[3][4] = -1.30290028722016;
     fWeightMatrix0to1[4][4] = -49.5975433863944;
     fWeightMatrix0to1[5][4] = -24.9254684302377;
     fWeightMatrix0to1[6][4] = 30.3398826436804;
     fWeightMatrix0to1[7][4] = -1.40266667371555;
     fWeightMatrix0to1[8][4] = 33.3763741231589;
     fWeightMatrix0to1[9][4] = 2.83709336083157;
     fWeightMatrix0to1[0][5] = 2.16633228579812;
     fWeightMatrix0to1[1][5] = 0.149316763261451;
     fWeightMatrix0to1[2][5] = -28.0510120484557;
     fWeightMatrix0to1[3][5] = 1.48947030290536;
     fWeightMatrix0to1[4][5] = 3.31909225272089;
     fWeightMatrix0to1[5][5] = 1.50257870289561;
     fWeightMatrix0to1[6][5] = -6.42093408725713;
     fWeightMatrix0to1[7][5] = -4.01613499600723;
     fWeightMatrix0to1[8][5] = 22.5853569887894;
     fWeightMatrix0to1[9][5] = -0.988703229599317;
     fWeightMatrix0to1[0][6] = -0.515475069093284;
     fWeightMatrix0to1[1][6] = 0.107873491456676;
     fWeightMatrix0to1[2][6] = 3.7896539966571;
     fWeightMatrix0to1[3][6] = -1.92206830502861;
     fWeightMatrix0to1[4][6] = 18.4492434520554;
     fWeightMatrix0to1[5][6] = -31.3178255977452;
     fWeightMatrix0to1[6][6] = 4.37461684624034;
     fWeightMatrix0to1[7][6] = 3.42005751481484;
     fWeightMatrix0to1[8][6] = -7.52309200432279;
     fWeightMatrix0to1[9][6] = 1.16121311781456;
     fWeightMatrix0to1[0][7] = 5.69099525042923;
     fWeightMatrix0to1[1][7] = -0.709077849551806;
     fWeightMatrix0to1[2][7] = -16.3792775664682;
     fWeightMatrix0to1[3][7] = 6.82749219677059;
     fWeightMatrix0to1[4][7] = -5.68770379136056;
     fWeightMatrix0to1[5][7] = -8.26036569859038;
     fWeightMatrix0to1[6][7] = -3.79213899618358;
     fWeightMatrix0to1[7][7] = -10.6438062975721;
     fWeightMatrix0to1[8][7] = 8.20014457995756;
     fWeightMatrix0to1[9][7] = 8.20255329829369;
     // weight matrix from layer 1 to 2
     fWeightMatrix1to2[0][0] = -1.12084468217478;
     fWeightMatrix1to2[1][0] = -1.42097522295155;
     fWeightMatrix1to2[2][0] = 0.0782684537101741;
     fWeightMatrix1to2[3][0] = -0.975849184350193;
     fWeightMatrix1to2[4][0] = -1.36256041673412;
     fWeightMatrix1to2[5][0] = -0.263140244746088;
     fWeightMatrix1to2[6][0] = -1.21197406229234;
     fWeightMatrix1to2[7][0] = 0.693466224402045;
     fWeightMatrix1to2[8][0] = 1.08487246925391;
     fWeightMatrix1to2[0][1] = -0.0446782697496722;
     fWeightMatrix1to2[1][1] = -1.67332132886065;
     fWeightMatrix1to2[2][1] = -0.935966205565634;
     fWeightMatrix1to2[3][1] = -0.711903382557495;
     fWeightMatrix1to2[4][1] = -2.52251525746623;
     fWeightMatrix1to2[5][1] = -2.53192420411418;
     fWeightMatrix1to2[6][1] = 0.0831796043408627;
     fWeightMatrix1to2[7][1] = -2.78890019799941;
     fWeightMatrix1to2[8][1] = 1.03799008336373;
     fWeightMatrix1to2[0][2] = -0.224480195037057;
     fWeightMatrix1to2[1][2] = 0.610785468341552;
     fWeightMatrix1to2[2][2] = -2.92188801884268;
     fWeightMatrix1to2[3][2] = -0.774161656278927;
     fWeightMatrix1to2[4][2] = 0.29591419521103;
     fWeightMatrix1to2[5][2] = 0.160621529223216;
     fWeightMatrix1to2[6][2] = -7.28313581268633;
     fWeightMatrix1to2[7][2] = 0.896688799158805;
     fWeightMatrix1to2[8][2] = -0.255417479930132;
     fWeightMatrix1to2[0][3] = -1.17399857513146;
     fWeightMatrix1to2[1][3] = -1.49788780317135;
     fWeightMatrix1to2[2][3] = -2.85320287621488;
     fWeightMatrix1to2[3][3] = -0.654462509613729;
     fWeightMatrix1to2[4][3] = -0.460201028616304;
     fWeightMatrix1to2[5][3] = -0.91892110249576;
     fWeightMatrix1to2[6][3] = 0.408582299903928;
     fWeightMatrix1to2[7][3] = -2.13617351122212;
     fWeightMatrix1to2[8][3] = 0.846400463160404;
     fWeightMatrix1to2[0][4] = 1.18274157694714;
     fWeightMatrix1to2[1][4] = 0.363577818975712;
     fWeightMatrix1to2[2][4] = -2.78648793907969;
     fWeightMatrix1to2[3][4] = -0.605550298034862;
     fWeightMatrix1to2[4][4] = -3.56776391965204;
     fWeightMatrix1to2[5][4] = -0.824338706374845;
     fWeightMatrix1to2[6][4] = -0.0739282265369494;
     fWeightMatrix1to2[7][4] = -0.617919855531056;
     fWeightMatrix1to2[8][4] = -1.26441402791213;
     fWeightMatrix1to2[0][5] = -7.85422121839153;
     fWeightMatrix1to2[1][5] = 0.649212928279747;
     fWeightMatrix1to2[2][5] = -1.49382735672959;
     fWeightMatrix1to2[3][5] = -1.163806181279;
     fWeightMatrix1to2[4][5] = -0.706737257250919;
     fWeightMatrix1to2[5][5] = -0.320001669951044;
     fWeightMatrix1to2[6][5] = -0.915244188961159;
     fWeightMatrix1to2[7][5] = -0.404929788423475;
     fWeightMatrix1to2[8][5] = -0.869837982881825;
     fWeightMatrix1to2[0][6] = 0.388461702018911;
     fWeightMatrix1to2[1][6] = 0.82417411752276;
     fWeightMatrix1to2[2][6] = -1.16986480480305;
     fWeightMatrix1to2[3][6] = -0.594234505980216;
     fWeightMatrix1to2[4][6] = 1.01190529478994;
     fWeightMatrix1to2[5][6] = 0.488157766444359;
     fWeightMatrix1to2[6][6] = 1.21927172754306;
     fWeightMatrix1to2[7][6] = 1.03555206522933;
     fWeightMatrix1to2[8][6] = -0.432120285066959;
     fWeightMatrix1to2[0][7] = -0.720718150886782;
     fWeightMatrix1to2[1][7] = -0.772587109315925;
     fWeightMatrix1to2[2][7] = 4.961492917941;
     fWeightMatrix1to2[3][7] = -2.07551955936462;
     fWeightMatrix1to2[4][7] = -1.49050888933505;
     fWeightMatrix1to2[5][7] = -5.18104528023642;
     fWeightMatrix1to2[6][7] = -3.41535949506006;
     fWeightMatrix1to2[7][7] = -3.63937612748511;
     fWeightMatrix1to2[8][7] = -0.0969639864582296;
     fWeightMatrix1to2[0][8] = 0.118654098736673;
     fWeightMatrix1to2[1][8] = 0.643045299775251;
     fWeightMatrix1to2[2][8] = -6.68887454121865;
     fWeightMatrix1to2[3][8] = -0.844473100072775;
     fWeightMatrix1to2[4][8] = 1.65222133238673;
     fWeightMatrix1to2[5][8] = 0.770190997903331;
     fWeightMatrix1to2[6][8] = 1.28375929668021;
     fWeightMatrix1to2[7][8] = -0.00194208275099416;
     fWeightMatrix1to2[8][8] = -0.531798783461306;
     fWeightMatrix1to2[0][9] = 0.293772101944011;
     fWeightMatrix1to2[1][9] = 0.476087002004467;
     fWeightMatrix1to2[2][9] = -2.86339487514251;
     fWeightMatrix1to2[3][9] = -0.106973459039663;
     fWeightMatrix1to2[4][9] = 0.237379536635383;
     fWeightMatrix1to2[5][9] = -6.55306519686434;
     fWeightMatrix1to2[6][9] = 0.205897405036469;
     fWeightMatrix1to2[7][9] = 0.795064632110721;
     fWeightMatrix1to2[8][9] = -0.962009195010732;
     fWeightMatrix1to2[0][10] = 2.4000326297059;
     fWeightMatrix1to2[1][10] = 3.79579237455207;
     fWeightMatrix1to2[2][10] = 7.69085292477427;
     fWeightMatrix1to2[3][10] = 7.36111070439386;
     fWeightMatrix1to2[4][10] = 1.85118194058879;
     fWeightMatrix1to2[5][10] = -0.0521258745313225;
     fWeightMatrix1to2[6][10] = 1.83688685910519;
     fWeightMatrix1to2[7][10] = -2.36525257885871;
     fWeightMatrix1to2[8][10] = 3.76587549476592;
     // weight matrix from layer 2 to 3
     fWeightMatrix2to3[0][0] = -1.16975284882838;
     fWeightMatrix2to3[1][0] = 0.182436124863262;
     fWeightMatrix2to3[2][0] = 1.32004139776896;
     fWeightMatrix2to3[3][0] = -0.813332692867775;
     fWeightMatrix2to3[4][0] = -0.407974392591066;
     fWeightMatrix2to3[5][0] = -0.298095908286076;
     fWeightMatrix2to3[6][0] = -0.209551158952576;
     fWeightMatrix2to3[0][1] = 0.0591791985821578;
     fWeightMatrix2to3[1][1] = 0.496981191089355;
     fWeightMatrix2to3[2][1] = 0.125152050979222;
     fWeightMatrix2to3[3][1] = -1.06495030957181;
     fWeightMatrix2to3[4][1] = -0.548374906025986;
     fWeightMatrix2to3[5][1] = -0.124331842255717;
     fWeightMatrix2to3[6][1] = 1.50571085321915;
     fWeightMatrix2to3[0][2] = -0.50305998401584;
     fWeightMatrix2to3[1][2] = 1.11074917712974;
     fWeightMatrix2to3[2][2] = -0.187830512893655;
     fWeightMatrix2to3[3][2] = -0.796139378341234;
     fWeightMatrix2to3[4][2] = 0.156072974019272;
     fWeightMatrix2to3[5][2] = -0.0467995879623844;
     fWeightMatrix2to3[6][2] = 0.0265475889574797;
     fWeightMatrix2to3[0][3] = 0.180573513036599;
     fWeightMatrix2to3[1][3] = -0.392574059691424;
     fWeightMatrix2to3[2][3] = -0.56129662210301;
     fWeightMatrix2to3[3][3] = 1.39644020937997;
     fWeightMatrix2to3[4][3] = 3.17536266287918;
     fWeightMatrix2to3[5][3] = 0.462627410049255;
     fWeightMatrix2to3[6][3] = -0.271723558819108;
     fWeightMatrix2to3[0][4] = 0.281470936587779;
     fWeightMatrix2to3[1][4] = -3.07088331788158;
     fWeightMatrix2to3[2][4] = -0.0813497289253693;
     fWeightMatrix2to3[3][4] = -0.347310938430066;
     fWeightMatrix2to3[4][4] = -0.763993695187927;
     fWeightMatrix2to3[5][4] = -1.94323588808135;
     fWeightMatrix2to3[6][4] = -0.398015524174699;
     fWeightMatrix2to3[0][5] = -0.91364566597847;
     fWeightMatrix2to3[1][5] = -0.259606356782924;
     fWeightMatrix2to3[2][5] = -1.25024846199983;
     fWeightMatrix2to3[3][5] = 0.685295486731914;
     fWeightMatrix2to3[4][5] = 2.66445157740408;
     fWeightMatrix2to3[5][5] = -1.70863318714099;
     fWeightMatrix2to3[6][5] = -0.56224569042501;
     fWeightMatrix2to3[0][6] = -0.0781893585389807;
     fWeightMatrix2to3[1][6] = -0.2157817435887;
     fWeightMatrix2to3[2][6] = -0.166414327646281;
     fWeightMatrix2to3[3][6] = -0.140533721077004;
     fWeightMatrix2to3[4][6] = -0.401772332808343;
     fWeightMatrix2to3[5][6] = -0.448814135432277;
     fWeightMatrix2to3[6][6] = 0.585911073797476;
     fWeightMatrix2to3[0][7] = -0.848544487609584;
     fWeightMatrix2to3[1][7] = -0.673594428005596;
     fWeightMatrix2to3[2][7] = -0.570735830368554;
     fWeightMatrix2to3[3][7] = -0.337810117449569;
     fWeightMatrix2to3[4][7] = -0.850964902420546;
     fWeightMatrix2to3[5][7] = -1.16577487918861;
     fWeightMatrix2to3[6][7] = 0.570034431102591;
     fWeightMatrix2to3[0][8] = -0.701396529760941;
     fWeightMatrix2to3[1][8] = 0.778845996680412;
     fWeightMatrix2to3[2][8] = -0.313119262292289;
     fWeightMatrix2to3[3][8] = 0.0538919388985925;
     fWeightMatrix2to3[4][8] = 0.0837696630186691;
     fWeightMatrix2to3[5][8] = -1.93455771407413;
     fWeightMatrix2to3[6][8] = -0.0478752668570304;
     fWeightMatrix2to3[0][9] = 3.78541028470798;
     fWeightMatrix2to3[1][9] = -1.72163006143859;
     fWeightMatrix2to3[2][9] = 0.603240734576226;
     fWeightMatrix2to3[3][9] = 2.80666479501064;
     fWeightMatrix2to3[4][9] = -11.6469071199572;
     fWeightMatrix2to3[5][9] = -2.08478104540014;
     fWeightMatrix2to3[6][9] = -3.80015882535704;
     // weight matrix from layer 3 to 4
     fWeightMatrix3to4[0][0] = -0.408291319489926;
     fWeightMatrix3to4[0][1] = 0.433421843843804;
     fWeightMatrix3to4[0][2] = -0.45970598445461;
     fWeightMatrix3to4[0][3] = 0.531229831434917;
     fWeightMatrix3to4[0][4] = 0.482734358752872;
     fWeightMatrix3to4[0][5] = 0.975569090219759;
     fWeightMatrix3to4[0][6] = -0.450251757645731;
     fWeightMatrix3to4[0][7] = 0.4271524095352;
  }

  inline float GetMvaValue__( const std::vector<float>& inputValues ) const
  {
    //TODO check auto vectorization here: 'not vectorized: unsupported use in stmt' , 'Unsupported pattern'

     float fWeights0[8];
     float fWeights1[11];
     float fWeights2[10];
     float fWeights3[8];
     float fWeights4[1];

     for (int i=0; i<fLayerSize[0]; i++) fWeights0[i]=0.f;
     for (int i=0; i<fLayerSize[1]; i++) fWeights1[i]=0.f;
     for (int i=0; i<fLayerSize[2]; i++) fWeights2[i]=0.f;
     for (int i=0; i<fLayerSize[3]; i++) fWeights3[i]=0.f;
     for (int i=0; i<fLayerSize[4]; i++) fWeights4[i]=0.f;

     fWeights0[fLayerSize[0]-1]=1.f;
     fWeights1[fLayerSize[1]-1]=1.f;
     fWeights2[fLayerSize[2]-1]=1.f;
     fWeights3[fLayerSize[3]-1]=1.f;
     /// no offset node in output layer! i.e. no 1.f initialisation for fWeights4

     for (int i=0; i<7; i++)
        fWeights0[i]=inputValues[i];

     // layer 0 to 1
     for (int o=0; o<10; o++) {
        for (int i=0; i<8; i++) {
           float inputVal = fWeightMatrix0to1[o][i] * fWeights0[i];
           fWeights1[o] += inputVal;
        }
        fWeights1[o] = ActivationFnc(fWeights1[o]);
     }
     // layer 1 to 2
     for (int o=0; o<9; o++) {
        for (int i=0; i<11; i++) {
           float inputVal = fWeightMatrix1to2[o][i] * fWeights1[i];
           fWeights2[o] += inputVal;
        }
        fWeights2[o] = ActivationFnc(fWeights2[o]);
     }
     // layer 2 to 3
     for (int o=0; o<7; o++) {
        for (int i=0; i<10; i++) {
           float inputVal = fWeightMatrix2to3[o][i] * fWeights2[i];
           fWeights3[o] += inputVal;
        }
        fWeights3[o] = ActivationFnc(fWeights3[o]);
     }
     // layer 3 to 4
     for (int i=0; i<8; i++) {
       float inputVal = fWeightMatrix3to4[0][i] * fWeights3[i];
       fWeights4[0] += inputVal;
     }
     return OutputActivationFnc(fWeights4[0]);

  }

  inline float ActivationFnc(float x) const {
     // rectified linear unit
     return x*(x>0);
  }

  //TODO do we need exp here? can we live with pure x output?!
  inline float OutputActivationFnc(float x) const {
     // sigmoid
     return 1.0/(1.0+exp(-x));
  }


  int fLayers;
  int fLayerSize[5];
  float fWeightMatrix0to1[11][8];   // weight matrix from layer 0 to 1
  float fWeightMatrix1to2[10][11];   // weight matrix from layer 1 to 2
  float fWeightMatrix2to3[8][10];   // weight matrix from layer 2 to 3
  float fWeightMatrix3to4[1][8];   // weight matrix from layer 3 to 4

};
